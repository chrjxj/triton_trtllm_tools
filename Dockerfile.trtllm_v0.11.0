# SPDX-FileCopyrightText: Copyright (c) 2024 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
# SPDX-License-Identifier: Apache-2.0
#
# Licensed under the Apache License, Version 2.0 (the "License");
# you may not use this file except in compliance with the License.
# You may obtain a copy of the License at
#
# http://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an "AS IS" BASIS,
# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
# See the License for the specific language governing permissions and
# limitations under the License.

# you need to build triton_trt_llm:v0.10.0 image before using this Dockerfile
FROM nvcr.io/nvidian/sae/customer_triton_trt_llm:v0.11.0

ENV LD_LIBRARY_PATH=/opt/tritonserver/backends/tensorrtllm:$LD_LIBRARY_PATH

ENV SRC_CODE_VERSION=0.11.0

WORKDIR /opt
RUN wget https://github.com/triton-inference-server/tensorrtllm_backend/archive/refs/tags/v${SRC_CODE_VERSION}.zip -O /opt/tensorrtllm_backend.zip && \
    unzip tensorrtllm_backend.zip -d /opt/ && mv /opt/tensorrtllm_backend-${SRC_CODE_VERSION} /opt/tensorrtllm_backend && \
    wget https://github.com/NVIDIA/TensorRT-LLM/archive/refs/tags/v${SRC_CODE_VERSION}.zip -O tensorrt_llm.zip && \
    unzip tensorrt_llm.zip -d /opt/ && \
    mv /opt/TensorRT-LLM-${SRC_CODE_VERSION}  /opt/tensorrt_llm && \
    cp -r /opt/tensorrtllm_backend/scripts /opt/scripts && \
    cp -r /opt/tensorrtllm_backend/tools/fill_template.py /opt/scripts/fill_template.py && \
    rm -rf /opt/*.zip

COPY ./inference_deploy /opt/inference_deploy
COPY ./inference_perf /opt/inference_perf

RUN pip install tritonclient[all]==2.41.0

WORKDIR /opt
